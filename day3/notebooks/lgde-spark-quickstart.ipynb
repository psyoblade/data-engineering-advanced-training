{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 스파크 실습 환경\n",
    "> 스파크의 실습을 위한 환경 구성이 정상인지 확인\n",
    "\n",
    "## 1. 스파크 세션 생성\n",
    "> 세션 생성에 오류(ERROR) 없이 생성되는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/18 04:28:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 스파크 버전 생성\n",
    "> Spark 3.1.2 버전 출력이 정상적으로 나오는 지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.6\n",
      "spark.version: 3.1.2\n"
     ]
    }
   ],
   "source": [
    "# !which python\n",
    "!/opt/conda/bin/python --version\n",
    "print(\"spark.version: {}\".format((spark.version)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 PySpark 관련 팁\n",
    "\n",
    "### 여러 줄의 코드 작성\n",
    "* python 코드의 경우 괄호로 (python) 묶으면 이스케이핑(\\) 하지 않아도 됩니다\n",
    "* sql 문의 경우  \"\"\"sql\"\"\" 으로 묶으면 이스케이핑(\\)하지 않아도 됩니다\n",
    "\n",
    "### 데이터 출력 함수\n",
    "* DataFrame.show() - 기본 제공 함수이며, show(n=limit) 통하여 최대 출력을 직접 조정할 수 있으나, 한글 출력 시에 표가 깨지는 경우가 있습니다\n",
    "* display(DataFrame) - Ipython 함수이며, limit 출력을 위해서는 limit 를 걸어야 하지만, 한글 출력에도 깨지지 않습니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      "\n",
      "+------+--------+\n",
      "|emp_id|emp_name|\n",
      "+------+--------+\n",
      "|     1|엘지전자|\n",
      "|     2|엘지화학|\n",
      "+------+--------+\n",
      "\n",
      "+------+\n",
      "|emp_id|\n",
      "+------+\n",
      "|     1|\n",
      "|     2|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th><th>emp_name</th></tr>\n",
       "<tr><td>1</td><td>엘지전자</td></tr>\n",
       "<tr><td>2</td><td>엘지화학</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+--------+\n",
       "|emp_id|emp_name|\n",
       "+------+--------+\n",
       "|     1|엘지전자|\n",
       "|     2|엘지화학|\n",
       "+------+--------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>emp_id</th></tr>\n",
       "<tr><td>1</td></tr>\n",
       "<tr><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+\n",
       "|emp_id|\n",
       "+------+\n",
       "|     1|\n",
       "|     2|\n",
       "+------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 파이썬 코드 여러 줄 작성\n",
    "json = (\n",
    "    spark\n",
    "    .read\n",
    "    .json(f\"{work_data}/tmp/simple.json\")\n",
    "    .limit(2)\n",
    ")\n",
    "\n",
    "## 스파크 SQL 여러 줄 작성\n",
    "json.createOrReplaceTempView(\"simple\")\n",
    "spark.sql(\"\"\"\n",
    "    select * \n",
    "    from simple\n",
    "\"\"\")\n",
    "\n",
    "json.printSchema()\n",
    "emp_id = json.select(\"emp_id\")\n",
    "\n",
    "## 표준 데이터 출력함수\n",
    "json.show()\n",
    "emp_id.show()\n",
    "\n",
    "## 노트북 출력함수 \n",
    "display(json)\n",
    "display(emp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |   simple|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컨테이너 정보 확인\n",
    "> 컨테이너 내부에 존재하는 파일 등에 대해 직접 접근이 가능합니다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|value   |\n",
      "+--------+\n",
      "|boto3   |\n",
      "|scrapy  |\n",
      "|selenium|\n",
      "|mrjob   |\n",
      "|pyspark |\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "count of word is 9\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strings = spark.read.text(f\"{home_jovyan}/requirements.txt\")\n",
    "strings.show(5, truncate=False)\n",
    "count = strings.count()\n",
    "print(\"count of word is {}\".format(count))\n",
    "\n",
    "strings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 데이터 읽기\n",
    "> 아무런 옵션을 주지 않는 경우 스파크가 알아서 컬럼 이름과 데이터 타입을 (string) 지정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n",
      "+----------+-----+------+\n",
      "|       _c0|  _c1|   _c2|\n",
      "+----------+-----+------+\n",
      "|    a_time|a_uid|  a_id|\n",
      "|1603645200|    1| login|\n",
      "|1603647200|    1|logout|\n",
      "|1603649200|    2| login|\n",
      "|1603650200|    2|logout|\n",
      "|1603653200|    2| login|\n",
      "|1603657200|    3| login|\n",
      "|1603659200|    3|logout|\n",
      "|1603660200|    4| login|\n",
      "|1603664200|    4|logout|\n",
      "|1603664500|    4| login|\n",
      "|1603666500|    5| login|\n",
      "|1603669500|    5|logout|\n",
      "|1603670500|    6| login|\n",
      "|1603673500|    7| login|\n",
      "|1603674500|    8| login|\n",
      "|1603675500|    9| login|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_access = spark.read.csv(f\"{work_data}/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 첫 번째 라인에 헤더가 포함되어 있는 경우 아래와 같이 header option 을 지정하면 컬럼 명을 가져올 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a_time: string (nullable = true)\n",
      " |-- a_uid: string (nullable = true)\n",
      " |-- a_id: string (nullable = true)\n",
      "\n",
      "+----------+-----+------+\n",
      "|    a_time|a_uid|  a_id|\n",
      "+----------+-----+------+\n",
      "|1603645200|    1| login|\n",
      "|1603647200|    1|logout|\n",
      "|1603649200|    2| login|\n",
      "|1603650200|    2|logout|\n",
      "|1603653200|    2| login|\n",
      "|1603657200|    3| login|\n",
      "|1603659200|    3|logout|\n",
      "|1603660200|    4| login|\n",
      "|1603664200|    4|logout|\n",
      "|1603664500|    4| login|\n",
      "|1603666500|    5| login|\n",
      "|1603669500|    5|logout|\n",
      "|1603670500|    6| login|\n",
      "|1603673500|    7| login|\n",
      "|1603674500|    8| login|\n",
      "|1603675500|    9| login|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_access = spark.read.option(\"header\", \"true\").csv(f\"{work_data}/log_access.csv\")\n",
    "log_access.printSchema()\n",
    "log_access.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 스파크 애플리케이션\n",
    "\n",
    "| 구분 | 설명 | 기타 |\n",
    "|---|---|---|\n",
    "| Application | 스파크 프레임워크를 통해 빌드한 프로그램. 전체 작업을 관리하는 Driver 와 Executors 상에서 수행되는 프로그램으로 구분합니다 | - |\n",
    "| SparkSession | 스파크의 모든 기능을 사용하기 위해 생성하는 객체 | - |\n",
    "| Job | 하나의 액션(save, collect 등)을 수행하기 위해 여러개의 타스크로 구성된 병렬처리 단위 | DAG 혹은 Spark Execution Plan |\n",
    "| Stage | 하나의 잡은 다수의 스테이지라는 것으로 구성되며, 하나의 스테이지는 다수의 타스크 들로 구성됩니다 | - |\n",
    "| Task | 스파크 익스큐터에 보내지는 하나의 작업 단위 | 하나의 Core 혹은 Partition 단위의 작업 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. 스파크 UI\n",
    "> Default 포트는 4040 이므로 http://localhost:4040 에 접속"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 파이썬 vs 스파크\n",
    "\n",
    "### 파이썬을 이용하여 1에서 100까지 더하는 함수를 계산합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = 0\n",
    "for number in range(1, 101, 1): result += number\n",
    "print(result)\n",
    "\n",
    "# 파이썬 3.0 에서는 reduce 함수를 사용할 수 있습니다\n",
    "from functools import reduce \n",
    "reduce(lambda x, y: x + y, range(101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스파크 Structured API 를 통해서 1 ~ 100 까지 더하는 함수를 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5050\n",
      "5050\n"
     ]
    }
   ],
   "source": [
    "from operator import add  # 파이썬의 operator 의 add 함수를 그대로 사용합니다.\n",
    "sc = spark.sparkContext\n",
    "parallels = sc.parallelize((range(1, 101, 1))).reduce(add)  # 1 ~ 101 이전까지 1씩 증가하는 숫자를 분산객체인 RDD를 반드시 생성해야 여러 노드의 메모리에 객체가 생성됩니다.\n",
    "print(parallels)\n",
    "\n",
    "x = sc.parallelize((range(1, 101, 1))).reduce(lambda x,y: x+y)  # 파이썬 람다함수를 이용해서 익명함수를 직접 생성해서 전달해도 결과는 동일합니다\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기타 날짜 관련 함수\n",
    "> bigint 값인 날짜는 아래와 같이 from_unixtime 및 to_timestamp 함수를 통해 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----------+\n",
      "| Arrival_Time|    String_Datetime|String_Date|\n",
      "+-------------+-------------------+-----------+\n",
      "|1424686735175|2015-02-23 19:18:55| 2015-02-23|\n",
      "|1424686735377|2015-02-23 19:18:55| 2015-02-23|\n",
      "|1424686735577|2015-02-23 19:18:55| 2015-02-23|\n",
      "|1424686735776|2015-02-23 19:18:55| 2015-02-23|\n",
      "|1424686735979|2015-02-23 19:18:55| 2015-02-23|\n",
      "+-------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_timestamp, to_date\n",
    "timestamp = df.select(\n",
    "    \"Arrival_Time\",\n",
    "    to_timestamp(from_unixtime(col('Arrival_Time') / lit(1000)), 'yyyy-MM-dd HH:mm:ss').alias('String_Datetime'),\n",
    "    to_date(from_unixtime(col('Arrival_Time') / lit(1000)), 'yyyy-MM-dd HH:mm:ss').alias('String_Date')\n",
    ")\n",
    "timestamp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
